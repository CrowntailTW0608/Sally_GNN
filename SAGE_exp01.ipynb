{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327344d3-dde4-4160-b05a-1b85d8c8aac1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime \n",
    "import glob\n",
    "pd.set_option('display.max_rows', 110)\n",
    "\n",
    "\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.utils import dense_to_sparse, to_dense_adj\n",
    "from scipy.spatial import distance\n",
    "# Contruct a two-layer GNN model\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8297b83c-e483-4bc2-9d5d-6ed7fa49682f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download dataset from : https://data.epa.gov.tw/dataset/aqx_p_488\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "# for r,ds,fs in os.walk(r'C:\\Users\\crown\\OneDrive\\桌面\\新增資料夾\\2021\\aqx_p_488_2021-07'):\n",
    "#     fs = [f for f in fs if f.endswith('.csv')]\n",
    "#     for f in fs:\n",
    "#         p = os.path.join(r,f)\n",
    "#         print(p)\n",
    "        \n",
    "#         df_tmp = pd.read_csv(p)\n",
    "        \n",
    "#         df = df.append(df_tmp)\n",
    "        \n",
    "#         del df_tmp\n",
    "        \n",
    "# df.to_csv(r'./data/df_2021_07.csv',index=False,encoding='ansi')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80aa6e45-1604-41a6-85d0-0981add09ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PM25Dataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='PM25')\n",
    "\n",
    "    def process(self):\n",
    "        \n",
    "        df = self.gen_df()\n",
    "        self.nodes_data = df\n",
    "        col_features=[ 'SO2', 'CO', 'O3', 'O3_8hr', 'PM10', 'NO2', 'NOx', 'NO', 'WindSpeed', 'WindDirec', 'CO_8hr', 'PM2.5_AVG', 'PM10_AVG', 'SO2_AVG']\n",
    "        node_features = torch.from_numpy(df[col_features].to_numpy())\n",
    "        node_features = (node_features-node_features.mean(axis=(0,1)))/node_features.std(axis=(0,1))\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float32)\n",
    "        node_labels = torch.from_numpy(df[['PM2.5']].to_numpy())\n",
    "        node_labels = torch.tensor(node_labels, dtype=torch.float32)\n",
    "        \n",
    "        edge_index, dist = self.gen_edge()\n",
    "        \n",
    "        edge_features = torch.from_numpy(dist)\n",
    "        edges_src = torch.from_numpy(edge_index[0])\n",
    "        edges_dst = torch.from_numpy(edge_index[1])\n",
    "\n",
    "        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=self.nodes_data.shape[0])\n",
    "        self.graph.ndata['feat'] = node_features\n",
    "        self.graph.ndata['label'] = node_labels\n",
    "        self.graph.edata['weight'] = edge_features\n",
    "\n",
    "        # If your dataset is a node classification dataset, you will need to assign\n",
    "        # masks indicating whether a node belongs to training, validation, and test set.\n",
    "        n_nodes = self.nodes_data.shape[0]\n",
    "        n_train = int(n_nodes * 0.6)\n",
    "        n_val = int(n_nodes * 0.2)\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train:n_train + n_val] = True\n",
    "        test_mask[n_train + n_val:] = True\n",
    "        self.graph.ndata['train_mask'] = train_mask\n",
    "        self.graph.ndata['val_mask'] = val_mask\n",
    "        self.graph.ndata['test_mask'] = test_mask\n",
    "\n",
    "    def gen_df(self):\n",
    "        df = pd.read_csv(r'./data/df_2021_07.csv',encoding='ansi')\n",
    "        df = df[['SO2', 'CO', 'O3', 'O3_8hr', 'PM10', 'NO2', 'NOx', 'NO', 'WindSpeed', 'WindDirec', 'CO_8hr', 'PM2.5_AVG', 'PM10_AVG', 'SO2_AVG','PM2.5','Longitude','Latitude']]\n",
    "        \n",
    "        df = df.replace({'-':0}).astype(float).dropna()\n",
    "        df[['Longitude','Latitude']] = df[['Longitude','Latitude']].apply(lambda x:round(x,5))\n",
    "\n",
    "        from sklearn.utils import shuffle\n",
    "        df = shuffle(df)\n",
    "        return df\n",
    "    \n",
    "    def gen_edge(self):\n",
    "        \n",
    "        node_num = self.nodes_data.shape[0]\n",
    "        coords = []\n",
    "        for i,r in self.nodes_data.iterrows():\n",
    "            coords.append([r['Longitude'], r['Latitude']])\n",
    "\n",
    "        dist = distance.cdist(coords, coords, 'euclidean')\n",
    "        adj = np.zeros((node_num, node_num), dtype=np.uint8)\n",
    "        adj[dist <= 0.4] = 1\n",
    "        assert adj.shape == dist.shape\n",
    "        dist = dist * adj #\n",
    "        edge_index, dist = dense_to_sparse(torch.tensor(dist)) # convert to spare tensor\n",
    "        edge_index, dist = edge_index.numpy(), dist.numpy()\n",
    "\n",
    "        return edge_index, dist\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef76e265-81f7-41c1-bb64-24ddcc1daecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crown\\.conda\\envs\\yp37_pytorch170_GNN\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\crown\\.conda\\envs\\yp37_pytorch170_GNN\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=28607, num_edges=138648656,\n",
      "      ndata_schemes={'feat': Scheme(shape=(14,), dtype=torch.float32), 'label': Scheme(shape=(1,), dtype=torch.float32), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
      "      edata_schemes={'weight': Scheme(shape=(), dtype=torch.float64)})\n"
     ]
    }
   ],
   "source": [
    "dataset = PM25Dataset()\n",
    "graph = dataset[0]\n",
    "\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9a2d8a-a0af-4b7d-83d2-17f6382cde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SAGEConv( in_feats=in_feats, \n",
    "                                    out_feats=hid_feats, \n",
    "                                    aggregator_type='mean')\n",
    "        self.conv2 = dglnn.SAGEConv( in_feats=hid_feats, \n",
    "                                    out_feats=hid_feats, \n",
    "                                    aggregator_type='mean')\n",
    "        self.conv3 = dglnn.SAGEConv( in_feats=hid_feats, \n",
    "                                    out_feats=out_feats, \n",
    "                                    aggregator_type='mean')\n",
    "        self.linear1 = nn.Linear(out_feats,1)\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(graph, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv3(graph, h)\n",
    "        h = self.linear1(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e181f965-cb63-42d2-939d-cf90ceaac73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = graph.ndata['feat']\n",
    "node_labels = graph.ndata['label']\n",
    "train_mask = graph.ndata['train_mask']\n",
    "valid_mask = graph.ndata['val_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "n_features = node_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa3e792-fa6c-4fa2-a6fe-1e12221843b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graph, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        loss = F.mse_loss(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed55f33-1a2d-433d-aa4c-39c00ef3b4af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SAGE(in_feats=n_features, hid_feats=64, out_feats=16)\n",
    "opt = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "train_losses,val_losses = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784d015d-4cca-4851-b083-38b34e137f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0, train loss:121.7462 val loss:37.2726\n",
      "#1, train loss:38.2556 val loss:45.9298 no improve...\n",
      "#2, train loss:47.2568 val loss:48.2730 no improve...\n",
      "#3, train loss:49.6166 val loss:34.1101\n",
      "#4, train loss:35.3089 val loss:28.9700\n",
      "#5, train loss:30.0462 val loss:31.1946 no improve...\n",
      "#6, train loss:32.2027 val loss:34.3053 no improve...\n",
      "#7, train loss:35.2823 val loss:35.1910 no improve...\n",
      "#8, train loss:36.1682 val loss:33.2811 no improve...\n",
      "#9, train loss:34.2737 val loss:29.5987 no improve...\n",
      "#10, train loss:30.6237 val loss:26.8052\n",
      "#11, train loss:27.8707 val loss:27.7537 no improve...\n",
      "#12, train loss:28.8661 val loss:29.0366 no improve...\n",
      "#13, train loss:30.1375 val loss:27.5395 no improve...\n",
      "#14, train loss:28.5907 val loss:25.3710\n",
      "#15, train loss:26.3565 val loss:24.3573\n",
      "#16, train loss:25.2808 val loss:24.2209\n",
      "#17, train loss:25.0921 val loss:24.0526\n",
      "#18, train loss:24.8777 val loss:23.4004\n",
      "#19, train loss:24.1901 val loss:22.4940\n",
      "#20, train loss:23.2528 val loss:21.8898\n",
      "#21, train loss:22.6168 val loss:21.8254\n",
      "#22, train loss:22.5175 val loss:21.3972\n",
      "#23, train loss:22.0457 val loss:20.4272\n",
      "#24, train loss:21.0309 val loss:19.7311\n",
      "#25, train loss:20.2845 val loss:19.4410\n",
      "#26, train loss:19.9389 val loss:19.0640\n",
      "#27, train loss:19.5097 val loss:18.4599\n",
      "#28, train loss:18.8638 val loss:18.0245\n",
      "#29, train loss:18.4025 val loss:17.8724\n",
      "#30, train loss:18.2346 val loss:17.4684\n",
      "#31, train loss:17.8032 val loss:17.0181\n",
      "#32, train loss:17.3111 val loss:16.8576\n",
      "#33, train loss:17.1101 val loss:16.6255\n",
      "#34, train loss:16.8550 val loss:16.1891\n",
      "#35, train loss:16.4203 val loss:15.8868\n",
      "#36, train loss:16.1390 val loss:15.6788\n",
      "#37, train loss:15.9452 val loss:15.2807\n",
      "#38, train loss:15.5282 val loss:14.9567\n",
      "#39, train loss:15.1729 val loss:14.6992\n",
      "#40, train loss:14.9052 val loss:14.3220\n",
      "#41, train loss:14.5497 val loss:14.0750\n",
      "#42, train loss:14.3604 val loss:13.8326\n",
      "#43, train loss:14.1266 val loss:13.5939\n",
      "#44, train loss:13.8519 val loss:13.4486\n",
      "#45, train loss:13.6978 val loss:13.1063\n",
      "#46, train loss:13.4035 val loss:12.9079\n",
      "#47, train loss:13.2674 val loss:12.6787\n",
      "#48, train loss:13.0352 val loss:12.5593\n",
      "#49, train loss:12.8803 val loss:12.3901\n",
      "#50, train loss:12.7208 val loss:12.1654\n",
      "#51, train loss:12.5514 val loss:12.0297\n",
      "#52, train loss:12.4321 val loss:11.8897\n",
      "#53, train loss:12.2436 val loss:11.7791\n",
      "#54, train loss:12.1207 val loss:11.5704\n",
      "#55, train loss:11.9636 val loss:11.4307\n",
      "#56, train loss:11.8322 val loss:11.3448\n",
      "#57, train loss:11.7026 val loss:11.1721\n",
      "#58, train loss:11.5490 val loss:11.0162\n",
      "#59, train loss:11.4396 val loss:10.8841\n",
      "#60, train loss:11.2825 val loss:10.7998\n",
      "#61, train loss:11.1824 val loss:10.6233\n",
      "#62, train loss:11.0440 val loss:10.5001\n",
      "#63, train loss:10.9317 val loss:10.4059\n",
      "#64, train loss:10.8037 val loss:10.2644\n",
      "#65, train loss:10.6679 val loss:10.1168\n",
      "#66, train loss:10.5490 val loss:9.9962\n",
      "#67, train loss:10.4057 val loss:9.8937\n",
      "#68, train loss:10.2873 val loss:9.7353\n",
      "#69, train loss:10.1557 val loss:9.6118\n",
      "#70, train loss:10.0220 val loss:9.5159\n",
      "#71, train loss:9.9061 val loss:9.3595\n",
      "#72, train loss:9.7757 val loss:9.2391\n",
      "#73, train loss:9.6498 val loss:9.1478\n",
      "#74, train loss:9.5395 val loss:9.0015\n",
      "#75, train loss:9.4260 val loss:8.9112\n",
      "#76, train loss:9.3043 val loss:8.7774\n",
      "#77, train loss:9.1837 val loss:8.6665\n",
      "#78, train loss:9.0683 val loss:8.5731\n",
      "#79, train loss:8.9574 val loss:8.4416\n",
      "#80, train loss:8.8501 val loss:8.3813\n",
      "#81, train loss:8.7495 val loss:8.2329\n",
      "#82, train loss:8.6507 val loss:8.2088\n",
      "#83, train loss:8.5571 val loss:8.0117\n",
      "#84, train loss:8.4332 val loss:7.9651\n",
      "#85, train loss:8.3102 val loss:7.7479\n",
      "#86, train loss:8.1607 val loss:7.6914\n",
      "#87, train loss:8.0318 val loss:7.5088\n",
      "#88, train loss:7.9182 val loss:7.5576 no improve...\n",
      "#89, train loss:7.8722 val loss:7.5244 no improve...\n",
      "#90, train loss:7.9774 val loss:8.0452 no improve...\n",
      "#91, train loss:8.2789 val loss:7.4502\n",
      "#92, train loss:7.9056 val loss:7.0669\n",
      "#93, train loss:7.3760 val loss:7.1027 no improve...\n",
      "#94, train loss:7.3771 val loss:7.1563 no improve...\n",
      "#95, train loss:7.5831 val loss:7.1743 no improve...\n",
      "#96, train loss:7.3955 val loss:6.7111\n",
      "#97, train loss:7.0122 val loss:6.8213 no improve...\n",
      "#98, train loss:7.1913 val loss:7.1724 no improve...\n",
      "#99, train loss:7.3397 val loss:6.5876\n",
      "#100, train loss:6.9008 val loss:6.5409\n",
      "#101, train loss:6.8473 val loss:6.9238 no improve...\n",
      "#102, train loss:7.0794 val loss:6.5090\n",
      "#103, train loss:6.8263 val loss:6.3540\n",
      "#104, train loss:6.6064 val loss:6.5361 no improve...\n",
      "#105, train loss:6.7215 val loss:6.4096 no improve...\n",
      "#106, train loss:6.7306 val loss:6.3368\n",
      "#107, train loss:6.5420 val loss:6.2183\n",
      "#108, train loss:6.4452 val loss:6.2413 no improve...\n",
      "#109, train loss:6.5366 val loss:6.3926 no improve...\n",
      "#110, train loss:6.5585 val loss:6.1271\n",
      "#111, train loss:6.3918 val loss:6.0785\n",
      "#112, train loss:6.3119 val loss:6.1969 no improve...\n",
      "#113, train loss:6.3727 val loss:6.1171 no improve...\n",
      "#114, train loss:6.3955 val loss:6.1479 no improve...\n",
      "#115, train loss:6.3227 val loss:5.9870\n",
      "#116, train loss:6.2195 val loss:5.9699\n",
      "#117, train loss:6.2076 val loss:6.0816 no improve...\n",
      "#118, train loss:6.2572 val loss:5.9987 no improve...\n",
      "#119, train loss:6.2633 val loss:6.0403 no improve...\n",
      "#120, train loss:6.2160 val loss:5.9114\n",
      "#121, train loss:6.1423 val loss:5.8977\n",
      "#122, train loss:6.1143 val loss:5.9478 no improve...\n",
      "#123, train loss:6.1327 val loss:5.9085 no improve...\n",
      "#124, train loss:6.1551 val loss:5.9920 no improve...\n",
      "#125, train loss:6.1595 val loss:5.8843\n",
      "#126, train loss:6.1245 val loss:5.9039 no improve...\n",
      "#127, train loss:6.0835 val loss:5.8421\n",
      "#128, train loss:6.0506 val loss:5.8371\n",
      "#129, train loss:6.0390 val loss:5.8684 no improve...\n",
      "#130, train loss:6.0446 val loss:5.8390 no improve...\n",
      "#131, train loss:6.0573 val loss:5.9182 no improve...\n",
      "#132, train loss:6.0721 val loss:5.8523 no improve...\n",
      "#133, train loss:6.0784 val loss:5.9445 no improve...\n",
      "#134, train loss:6.0896 val loss:5.8520 no improve...\n",
      "#135, train loss:6.0827 val loss:5.9343 no improve...\n",
      "#136, train loss:6.0761 val loss:5.8331\n",
      "#137, train loss:6.0461 val loss:5.8844 no improve...\n",
      "#138, train loss:6.0185 val loss:5.8000\n",
      "#139, train loss:5.9847 val loss:5.8124 no improve...\n",
      "#140, train loss:5.9594 val loss:5.7810\n",
      "#141, train loss:5.9436 val loss:5.7721\n",
      "#142, train loss:5.9382 val loss:5.7937 no improve...\n",
      "#143, train loss:5.9405 val loss:5.7666\n",
      "#144, train loss:5.9440 val loss:5.8180 no improve...\n",
      "#145, train loss:5.9482 val loss:5.7732 no improve...\n",
      "#146, train loss:5.9479 val loss:5.8393 no improve...\n",
      "#147, train loss:5.9508 val loss:5.7799 no improve...\n",
      "#148, train loss:5.9480 val loss:5.8482 no improve...\n",
      "#149, train loss:5.9492 val loss:5.7725 no improve...\n",
      "#150, train loss:5.9380 val loss:5.8284 no improve...\n",
      "#151, train loss:5.9300 val loss:5.7550\n",
      "#152, train loss:5.9148 val loss:5.7997 no improve...\n",
      "#153, train loss:5.9021 val loss:5.7385\n",
      "#154, train loss:5.8850 val loss:5.7694 no improve...\n",
      "#155, train loss:5.8712 val loss:5.7274\n",
      "#156, train loss:5.8565 val loss:5.7373 no improve...\n",
      "#157, train loss:5.8451 val loss:5.7214\n",
      "#158, train loss:5.8369 val loss:5.7169\n",
      "#159, train loss:5.8314 val loss:5.7268 no improve...\n",
      "#160, train loss:5.8285 val loss:5.7101\n",
      "#161, train loss:5.8273 val loss:5.7398 no improve...\n",
      "#162, train loss:5.8284 val loss:5.7104 no improve...\n",
      "#163, train loss:5.8321 val loss:5.7692 no improve...\n",
      "#164, train loss:5.8444 val loss:5.7333 no improve...\n",
      "#165, train loss:5.8661 val loss:5.8679 no improve...\n",
      "#166, train loss:5.9202 val loss:5.8338 no improve...\n",
      "#167, train loss:5.9899 val loss:6.1130 no improve...\n",
      "#168, train loss:6.1382 val loss:5.9939 no improve...\n",
      "#169, train loss:6.1795 val loss:6.1962 no improve...\n",
      "#170, train loss:6.2195 val loss:5.8062 no improve...\n",
      "#171, train loss:5.9609 val loss:5.7065\n",
      "#172, train loss:5.7900 val loss:5.7644 no improve...\n",
      "#173, train loss:5.8303 val loss:5.7954 no improve...\n",
      "#174, train loss:5.9450 val loss:5.9066 no improve...\n",
      "#175, train loss:5.9488 val loss:5.6843\n",
      "#176, train loss:5.8002 val loss:5.6710\n",
      "#177, train loss:5.7746 val loss:5.8221 no improve...\n",
      "#178, train loss:5.8678 val loss:5.7466 no improve...\n",
      "#179, train loss:5.8785 val loss:5.7541 no improve...\n",
      "#180, train loss:5.8093 val loss:5.6577\n",
      "#181, train loss:5.7464 val loss:5.6703 no improve...\n",
      "#182, train loss:5.7752 val loss:5.7938 no improve...\n",
      "#183, train loss:5.8345 val loss:5.7089 no improve...\n",
      "#184, train loss:5.8194 val loss:5.7154 no improve...\n",
      "#185, train loss:5.7661 val loss:5.6562\n",
      "#186, train loss:5.7308 val loss:5.6567 no improve...\n",
      "#187, train loss:5.7503 val loss:5.7384 no improve...\n",
      "#188, train loss:5.7840 val loss:5.6730 no improve...\n",
      "#189, train loss:5.7734 val loss:5.6861 no improve...\n",
      "#190, train loss:5.7386 val loss:5.6472\n",
      "#191, train loss:5.7172 val loss:5.6430\n",
      "#192, train loss:5.7312 val loss:5.7001 no improve...\n",
      "#193, train loss:5.7511 val loss:5.6498 no improve...\n",
      "#194, train loss:5.7414 val loss:5.6638 no improve...\n",
      "#195, train loss:5.7187 val loss:5.6367\n",
      "#196, train loss:5.7048 val loss:5.6271\n",
      "#197, train loss:5.7105 val loss:5.6655 no improve...\n",
      "#198, train loss:5.7226 val loss:5.6297 no improve...\n",
      "#199, train loss:5.7216 val loss:5.6564 no improve...\n",
      "#200, train loss:5.7105 val loss:5.6206\n",
      "#201, train loss:5.6964 val loss:5.6183\n",
      "#202, train loss:5.6908 val loss:5.6314 no improve...\n",
      "#203, train loss:5.6948 val loss:5.6166\n",
      "#204, train loss:5.7003 val loss:5.6524 no improve...\n",
      "#205, train loss:5.7029 val loss:5.6130\n",
      "#206, train loss:5.6963 val loss:5.6259 no improve...\n",
      "#207, train loss:5.6870 val loss:5.6021\n",
      "#208, train loss:5.6790 val loss:5.6036 no improve...\n",
      "#209, train loss:5.6753 val loss:5.6160 no improve...\n",
      "#210, train loss:5.6765 val loss:5.5995\n",
      "#211, train loss:5.6789 val loss:5.6229 no improve...\n",
      "#212, train loss:5.6803 val loss:5.5940\n",
      "#213, train loss:5.6784 val loss:5.6177 no improve...\n",
      "#214, train loss:5.6744 val loss:5.5904\n",
      "#215, train loss:5.6688 val loss:5.6016 no improve...\n",
      "#216, train loss:5.6635 val loss:5.5853\n",
      "#217, train loss:5.6592 val loss:5.5891 no improve...\n",
      "#218, train loss:5.6560 val loss:5.5871 no improve...\n",
      "#219, train loss:5.6537 val loss:5.5815\n",
      "#220, train loss:5.6522 val loss:5.5875 no improve...\n",
      "#221, train loss:5.6510 val loss:5.5756\n",
      "#222, train loss:5.6503 val loss:5.5893 no improve...\n",
      "#223, train loss:5.6501 val loss:5.5688\n",
      "#224, train loss:5.6501 val loss:5.5913 no improve...\n",
      "#225, train loss:5.6510 val loss:5.5684\n",
      "#226, train loss:5.6529 val loss:5.6065 no improve...\n",
      "#227, train loss:5.6577 val loss:5.5725 no improve...\n",
      "#228, train loss:5.6643 val loss:5.6344 no improve...\n",
      "#229, train loss:5.6797 val loss:5.5933 no improve...\n",
      "#230, train loss:5.6973 val loss:5.7067 no improve...\n",
      "#231, train loss:5.7370 val loss:5.6514 no improve...\n",
      "#232, train loss:5.7722 val loss:5.8245 no improve...\n",
      "#233, train loss:5.8374 val loss:5.7058 no improve...\n",
      "#234, train loss:5.8387 val loss:5.8293 no improve...\n",
      "#235, train loss:5.8407 val loss:5.6290 no improve...\n",
      "#236, train loss:5.7447 val loss:5.6200 no improve...\n",
      "#237, train loss:5.6621 val loss:5.5523\n",
      "#238, train loss:5.6205 val loss:5.5561 no improve...\n",
      "#239, train loss:5.6426 val loss:5.6642 no improve...\n",
      "#240, train loss:5.6939 val loss:5.6029 no improve...\n",
      "#241, train loss:5.7111 val loss:5.6730 no improve...\n",
      "#242, train loss:5.7004 val loss:5.5558 no improve...\n",
      "#243, train loss:5.6505 val loss:5.5586 no improve...\n",
      "#244, train loss:5.6137 val loss:5.5414\n",
      "#245, train loss:5.6042 val loss:5.5355\n",
      "#246, train loss:5.6197 val loss:5.6072 no improve...\n",
      "#247, train loss:5.6448 val loss:5.5567 no improve...\n",
      "#248, train loss:5.6546 val loss:5.6194 no improve...\n",
      "#249, train loss:5.6524 val loss:5.5348\n",
      "#250, train loss:5.6271 val loss:5.5506 no improve...\n",
      "#251, train loss:5.6032 val loss:5.5186\n",
      "#252, train loss:5.5893 val loss:5.5140\n",
      "#253, train loss:5.5903 val loss:5.5493 no improve...\n",
      "#254, train loss:5.6006 val loss:5.5195 no improve...\n",
      "#255, train loss:5.6096 val loss:5.5731 no improve...\n",
      "#256, train loss:5.6147 val loss:5.5182 no improve...\n",
      "#257, train loss:5.6088 val loss:5.5541 no improve...\n",
      "#258, train loss:5.6003 val loss:5.5059\n",
      "#259, train loss:5.5886 val loss:5.5239 no improve...\n",
      "#260, train loss:5.5791 val loss:5.5017\n",
      "#261, train loss:5.5722 val loss:5.5042 no improve...\n",
      "#262, train loss:5.5687 val loss:5.5080 no improve...\n",
      "#263, train loss:5.5678 val loss:5.4956\n",
      "#264, train loss:5.5689 val loss:5.5197 no improve...\n",
      "#265, train loss:5.5716 val loss:5.4936\n",
      "#266, train loss:5.5746 val loss:5.5322 no improve...\n",
      "#267, train loss:5.5790 val loss:5.4948 no improve...\n",
      "#268, train loss:5.5820 val loss:5.5464 no improve...\n",
      "#269, train loss:5.5875 val loss:5.5000 no improve...\n",
      "#270, train loss:5.5911 val loss:5.5642 no improve...\n",
      "#271, train loss:5.5998 val loss:5.5086 no improve...\n",
      "#272, train loss:5.6055 val loss:5.5920 no improve...\n",
      "#273, train loss:5.6215 val loss:5.5258 no improve...\n",
      "#274, train loss:5.6293 val loss:5.6289 no improve...\n",
      "#275, train loss:5.6488 val loss:5.5360 no improve...\n",
      "#276, train loss:5.6436 val loss:5.6242 no improve...\n",
      "#277, train loss:5.6456 val loss:5.5151 no improve...\n",
      "#278, train loss:5.6187 val loss:5.5646 no improve...\n",
      "#279, train loss:5.5953 val loss:5.4775\n",
      "#280, train loss:5.5630 val loss:5.4905 no improve...\n",
      "#281, train loss:5.5406 val loss:5.4648\n",
      "#282, train loss:5.5298 val loss:5.4598\n",
      "#283, train loss:5.5304 val loss:5.4917 no improve...\n",
      "#284, train loss:5.5393 val loss:5.4685 no improve...\n",
      "#285, train loss:5.5523 val loss:5.5376 no improve...\n",
      "#286, train loss:5.5697 val loss:5.4854 no improve...\n",
      "#287, train loss:5.5809 val loss:5.5717 no improve...\n",
      "#288, train loss:5.5973 val loss:5.4961 no improve...\n",
      "#289, train loss:5.5974 val loss:5.5778 no improve...\n",
      "#290, train loss:5.6000 val loss:5.4821 no improve...\n",
      "#291, train loss:5.5788 val loss:5.5243 no improve...\n",
      "#292, train loss:5.5577 val loss:5.4481\n",
      "#293, train loss:5.5317 val loss:5.4600 no improve...\n",
      "#294, train loss:5.5136 val loss:5.4400\n",
      "#295, train loss:5.5047 val loss:5.4345\n",
      "#296, train loss:5.5051 val loss:5.4660 no improve...\n",
      "#297, train loss:5.5131 val loss:5.4421 no improve...\n",
      "#298, train loss:5.5269 val loss:5.5214 no improve...\n",
      "#299, train loss:5.5519 val loss:5.4784 no improve...\n",
      "#300, train loss:5.5813 val loss:5.6182 no improve...\n",
      "#301, train loss:5.6311 val loss:5.5337 no improve...\n",
      "#302, train loss:5.6549 val loss:5.6918 no improve...\n",
      "#303, train loss:5.6967 val loss:5.5380 no improve...\n",
      "#304, train loss:5.6619 val loss:5.5994 no improve...\n",
      "#305, train loss:5.6166 val loss:5.4349 no improve...\n",
      "#306, train loss:5.5323 val loss:5.4241\n",
      "#307, train loss:5.4842 val loss:5.4259 no improve...\n",
      "#308, train loss:5.4850 val loss:5.4217\n",
      "#309, train loss:5.5197 val loss:5.5359 no improve...\n",
      "#310, train loss:5.5648 val loss:5.4631 no improve...\n",
      "#311, train loss:5.5746 val loss:5.5393 no improve...\n",
      "#312, train loss:5.5640 val loss:5.4187\n",
      "#313, train loss:5.5164 val loss:5.4268 no improve...\n",
      "#314, train loss:5.4780 val loss:5.3963\n",
      "#315, train loss:5.4621 val loss:5.3920\n",
      "#316, train loss:5.4720 val loss:5.4545 no improve...\n",
      "#317, train loss:5.4949 val loss:5.4097 no improve...\n",
      "#318, train loss:5.5095 val loss:5.4763 no improve...\n",
      "#319, train loss:5.5118 val loss:5.3964 no improve...\n",
      "#320, train loss:5.4922 val loss:5.4203 no improve...\n",
      "#321, train loss:5.4683 val loss:5.3780\n",
      "#322, train loss:5.4508 val loss:5.3778\n",
      "#323, train loss:5.4490 val loss:5.4087 no improve...\n",
      "#324, train loss:5.4572 val loss:5.3859 no improve...\n",
      "#325, train loss:5.4689 val loss:5.4470 no improve...\n",
      "#326, train loss:5.4787 val loss:5.3886 no improve...\n",
      "#327, train loss:5.4742 val loss:5.4279 no improve...\n",
      "#328, train loss:5.4649 val loss:5.3771\n",
      "#329, train loss:5.4522 val loss:5.3988 no improve...\n",
      "#330, train loss:5.4430 val loss:5.3735\n",
      "#331, train loss:5.4353 val loss:5.3786 no improve...\n",
      "#332, train loss:5.4313 val loss:5.3822 no improve...\n",
      "#333, train loss:5.4306 val loss:5.3720\n",
      "#334, train loss:5.4319 val loss:5.3985 no improve...\n",
      "#335, train loss:5.4352 val loss:5.3760 no improve...\n",
      "#336, train loss:5.4392 val loss:5.4212 no improve...\n",
      "#337, train loss:5.4457 val loss:5.3830 no improve...\n",
      "#338, train loss:5.4517 val loss:5.4435 no improve...\n",
      "#339, train loss:5.4587 val loss:5.3915 no improve...\n",
      "#340, train loss:5.4628 val loss:5.4575 no improve...\n",
      "#341, train loss:5.4658 val loss:5.3927 no improve...\n",
      "#342, train loss:5.4630 val loss:5.4537 no improve...\n",
      "#343, train loss:5.4623 val loss:5.3884 no improve...\n",
      "#344, train loss:5.4570 val loss:5.4453 no improve...\n",
      "#345, train loss:5.4537 val loss:5.3823 no improve...\n",
      "#346, train loss:5.4458 val loss:5.4286 no improve...\n",
      "#347, train loss:5.4385 val loss:5.3741 no improve...\n",
      "#348, train loss:5.4297 val loss:5.4104 no improve... early stop...\n",
      "test loss : {} 5.549008846282959\n"
     ]
    }
   ],
   "source": [
    "early_stop = 15\n",
    "min_val_loss = 999999\n",
    "min_val_epoch = 0\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    logits = model(graph, node_features)\n",
    "    \n",
    "    train_loss = F.mse_loss(logits[train_mask], node_labels[train_mask])\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    train_loss.backward()\n",
    "    opt.step()\n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    val_loss = evaluate(model, graph, node_features, node_labels, valid_mask)\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    print('#{}, train loss:{:.4f} val loss:{:.4f}'.format(epoch,train_loss.item(),val_loss.item()),end='')\n",
    "    \n",
    "    if val_loss.item() < min_val_loss:\n",
    "        min_val_loss = val_loss.item()\n",
    "        min_val_epoch = epoch\n",
    "    else:\n",
    "        print(' no improve...',end='')\n",
    "        \n",
    "    if epoch >= min_val_epoch + early_stop :\n",
    "        print(' early stop...')\n",
    "        break\n",
    "        \n",
    "\n",
    "    print()\n",
    "    # Save model if necessary.  Omitted in this example.\n",
    "    \n",
    "test_loss = evaluate(model, graph, node_features, node_labels, test_mask)\n",
    "print('test loss : {}',test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d55e87fc-9bd5-46cb-b8a4-09e17b34426f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x271bb601fc8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses,label='train')\n",
    "plt.plot(val_losses,label='validation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4617c-467c-4fd0-8462-937499be20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.549008846282959"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
