{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327344d3-dde4-4160-b05a-1b85d8c8aac1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime \n",
    "import glob\n",
    "pd.set_option('display.max_rows', 110)\n",
    "\n",
    "\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.utils import dense_to_sparse, to_dense_adj\n",
    "from scipy.spatial import distance\n",
    "# Contruct a two-layer GNN model\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8297b83c-e483-4bc2-9d5d-6ed7fa49682f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download dataset from : https://data.epa.gov.tw/dataset/aqx_p_488\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "# for r,ds,fs in os.walk(r'C:\\Users\\crown\\OneDrive\\桌面\\新增資料夾\\2021\\aqx_p_488_2021-07'):\n",
    "#     fs = [f for f in fs if f.endswith('.csv')]\n",
    "#     for f in fs:\n",
    "#         p = os.path.join(r,f)\n",
    "#         print(p)\n",
    "        \n",
    "#         df_tmp = pd.read_csv(p)\n",
    "        \n",
    "#         df = df.append(df_tmp)\n",
    "        \n",
    "#         del df_tmp\n",
    "        \n",
    "# df.to_csv(r'./data/df_2021_07.csv',index=False,encoding='ansi')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80aa6e45-1604-41a6-85d0-0981add09ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PM25Dataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='PM25')\n",
    "\n",
    "    def process(self):\n",
    "        \n",
    "        df = self.gen_df()\n",
    "        self.nodes_data = df\n",
    "        col_features=[ 'SO2', 'CO', 'O3', 'O3_8hr', 'PM10', 'NO2', 'NOx', 'NO', 'WindSpeed', 'WindDirec', 'CO_8hr', 'PM2.5_AVG', 'PM10_AVG', 'SO2_AVG']\n",
    "        node_features = torch.from_numpy(df[col_features].to_numpy())\n",
    "        node_features = (node_features-node_features.mean(axis=(0,1)))/node_features.std(axis=(0,1))\n",
    "        node_features = torch.tensor(node_features, dtype=torch.float32)\n",
    "        node_labels = torch.from_numpy(df[['PM2.5']].to_numpy())\n",
    "        node_labels = torch.tensor(node_labels, dtype=torch.float32)\n",
    "        \n",
    "        edge_index, dist = self.gen_edge()\n",
    "        \n",
    "        edge_features = torch.from_numpy(dist)\n",
    "        edges_src = torch.from_numpy(edge_index[0])\n",
    "        edges_dst = torch.from_numpy(edge_index[1])\n",
    "\n",
    "        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=self.nodes_data.shape[0])\n",
    "        self.graph.ndata['feat'] = node_features\n",
    "        self.graph.ndata['label'] = node_labels\n",
    "        self.graph.edata['weight'] = edge_features\n",
    "\n",
    "        # If your dataset is a node classification dataset, you will need to assign\n",
    "        # masks indicating whether a node belongs to training, validation, and test set.\n",
    "        n_nodes = self.nodes_data.shape[0]\n",
    "        n_train = int(n_nodes * 0.6)\n",
    "        n_val = int(n_nodes * 0.2)\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train:n_train + n_val] = True\n",
    "        test_mask[n_train + n_val:] = True\n",
    "        self.graph.ndata['train_mask'] = train_mask\n",
    "        self.graph.ndata['val_mask'] = val_mask\n",
    "        self.graph.ndata['test_mask'] = test_mask\n",
    "\n",
    "    def gen_df(self):\n",
    "        df = pd.read_csv(r'./data/df_2021_07.csv',encoding='ansi')\n",
    "        df = df[['SO2', 'CO', 'O3', 'O3_8hr', 'PM10', 'NO2', 'NOx', 'NO', 'WindSpeed', 'WindDirec', 'CO_8hr', 'PM2.5_AVG', 'PM10_AVG', 'SO2_AVG','PM2.5','Longitude','Latitude']]\n",
    "        \n",
    "        df = df.replace({'-':0}).astype(float).dropna()\n",
    "        df[['Longitude','Latitude']] = df[['Longitude','Latitude']].apply(lambda x:round(x,5))\n",
    "\n",
    "        from sklearn.utils import shuffle\n",
    "        df = shuffle(df)\n",
    "        return df\n",
    "    \n",
    "    def gen_edge(self):\n",
    "        \n",
    "        node_num = self.nodes_data.shape[0]\n",
    "        coords = []\n",
    "        for i,r in self.nodes_data.iterrows():\n",
    "            coords.append([r['Longitude'], r['Latitude']])\n",
    "\n",
    "        dist = distance.cdist(coords, coords, 'euclidean')\n",
    "        adj = np.zeros((node_num, node_num), dtype=np.uint8)\n",
    "        adj[dist <= 0.4] = 1\n",
    "        assert adj.shape == dist.shape\n",
    "        dist = dist * adj #\n",
    "        edge_index, dist = dense_to_sparse(torch.tensor(dist)) # convert to spare tensor\n",
    "        edge_index, dist = edge_index.numpy(), dist.numpy()\n",
    "\n",
    "        return edge_index, dist\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef76e265-81f7-41c1-bb64-24ddcc1daecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crown\\.conda\\envs\\yp37_pytorch170_GNN\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\crown\\.conda\\envs\\yp37_pytorch170_GNN\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=28607, num_edges=138648656,\n",
      "      ndata_schemes={'feat': Scheme(shape=(14,), dtype=torch.float32), 'label': Scheme(shape=(1,), dtype=torch.float32), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
      "      edata_schemes={'weight': Scheme(shape=(), dtype=torch.float64)})\n"
     ]
    }
   ],
   "source": [
    "dataset = PM25Dataset()\n",
    "graph = dataset[0]\n",
    "\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9a2d8a-a0af-4b7d-83d2-17f6382cde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SAGEConv( in_feats=in_feats, out_feats=hid_feats, aggregator_type='gcn')\n",
    "        self.conv2 = dglnn.SAGEConv( in_feats=hid_feats, out_feats=hid_feats, aggregator_type='gcn')\n",
    "        self.conv3 = dglnn.SAGEConv( in_feats=hid_feats, out_feats=out_feats, aggregator_type='gcn')\n",
    "        self.linear1 = nn.Linear(out_feats,1)\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(graph, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv3(graph, h)\n",
    "        h = self.linear1(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e181f965-cb63-42d2-939d-cf90ceaac73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = graph.ndata['feat']\n",
    "node_labels = graph.ndata['label']\n",
    "train_mask = graph.ndata['train_mask']\n",
    "valid_mask = graph.ndata['val_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "n_features = node_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aa3e792-fa6c-4fa2-a6fe-1e12221843b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graph, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        loss = F.mse_loss(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fed55f33-1a2d-433d-aa4c-39c00ef3b4af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SAGE(in_feats=n_features, hid_feats=64, out_feats=16)\n",
    "opt = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "train_losses,val_losses = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784d015d-4cca-4851-b083-38b34e137f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0, train loss:107.1249 val loss:88.9624\n",
      "#1, train loss:86.1105 val loss:68.0923\n",
      "#2, train loss:65.4426 val loss:47.5781\n",
      "#3, train loss:45.1923 val loss:33.0786\n",
      "#4, train loss:31.0253 val loss:35.9204 no improve...\n",
      "#5, train loss:34.2560 val loss:47.2508 no improve...\n",
      "#6, train loss:45.7743 val loss:45.1037 no improve...\n",
      "#7, train loss:43.5898 val loss:37.2315 no improve...\n",
      "#8, train loss:35.5752 val loss:32.1031\n",
      "#9, train loss:30.2744 val loss:31.4452\n",
      "#10, train loss:29.4606 val loss:33.2829 no improve...\n",
      "#11, train loss:31.1817 val loss:35.4859 no improve...\n",
      "#12, train loss:33.3069 val loss:36.8513 no improve...\n",
      "#13, train loss:34.6322 val loss:36.8516 no improve...\n",
      "#14, train loss:34.6313 val loss:35.7447 no improve...\n",
      "#15, train loss:33.5532 val loss:33.9160 no improve...\n",
      "#16, train loss:31.7812 val loss:31.9692 no improve...\n",
      "#17, train loss:29.9148 val loss:30.5818\n",
      "#18, train loss:28.6334 val loss:30.4762\n",
      "#19, train loss:28.6493 val loss:31.4345 no improve...\n",
      "#20, train loss:29.7173 val loss:32.2482 no improve...\n",
      "#21, train loss:30.5901 val loss:31.8847 no improve...\n",
      "#22, train loss:30.2108 val loss:31.0016 no improve...\n",
      "#23, train loss:29.2493 val loss:30.2320\n",
      "#24, train loss:28.3991 val loss:29.9471\n",
      "#25, train loss:28.0333 val loss:30.1077 no improve...\n",
      "#26, train loss:28.1261 val loss:30.4011 no improve...\n",
      "#27, train loss:28.3743 val loss:30.4351 no improve...\n",
      "#28, train loss:28.3992 val loss:30.0454 no improve...\n",
      "#29, train loss:28.0374 val loss:29.3643\n",
      "#30, train loss:27.4120 val loss:28.7843\n",
      "#31, train loss:26.9139 val loss:28.6043\n",
      "#32, train loss:26.8480 val loss:28.8934 no improve...\n",
      "#33, train loss:27.2611 val loss:28.7681 no improve...\n",
      "#34, train loss:27.1326 val loss:28.4203\n",
      "#35, train loss:26.7036 val loss:28.2715\n",
      "#36, train loss:26.4725 val loss:28.3497 no improve...\n",
      "#37, train loss:26.4852 val loss:28.5291 no improve...\n",
      "#38, train loss:26.6216 val loss:28.4729 no improve...\n",
      "#39, train loss:26.5696 val loss:28.2044\n",
      "#40, train loss:26.3408 val loss:27.9256\n",
      "#41, train loss:26.1262 val loss:27.7948\n",
      "#42, train loss:26.0705 val loss:27.8110 no improve...\n",
      "#43, train loss:26.1473 val loss:27.7952 no improve...\n",
      "#44, train loss:26.1534 val loss:27.6708\n",
      "#45, train loss:26.0073 val loss:27.5669\n",
      "#46, train loss:25.8550 val loss:27.5998 no improve...\n",
      "#47, train loss:25.8343 val loss:27.6431 no improve...\n",
      "#48, train loss:25.8495 val loss:27.6437 no improve...\n",
      "#49, train loss:25.8439 val loss:27.4817\n",
      "#50, train loss:25.7159 val loss:27.3326\n",
      "#51, train loss:25.6175 val loss:27.2620\n",
      "#52, train loss:25.6011 val loss:27.2414\n",
      "#53, train loss:25.6161 val loss:27.1928\n",
      "#54, train loss:25.5705 val loss:27.1286\n",
      "#55, train loss:25.4799 val loss:27.1151\n",
      "#56, train loss:25.4279 val loss:27.1423 no improve...\n",
      "#57, train loss:25.4250 val loss:27.1321 no improve...\n",
      "#58, train loss:25.4078 val loss:27.0505\n",
      "#59, train loss:25.3457 val loss:26.9506\n",
      "#60, train loss:25.2831 val loss:26.8909\n",
      "#61, train loss:25.2613 val loss:26.8572\n",
      "#62, train loss:25.2480 val loss:26.8147\n",
      "#63, train loss:25.1995 val loss:26.7826\n",
      "#64, train loss:25.1414 val loss:26.7985 no improve...\n",
      "#65, train loss:25.1235 val loss:26.8206 no improve...\n",
      "#66, train loss:25.1272 val loss:26.7351\n",
      "#67, train loss:25.0665 val loss:26.6576\n",
      "#68, train loss:25.0248 val loss:26.6149\n",
      "#69, train loss:25.0124 val loss:26.5878\n",
      "#70, train loss:24.9964 val loss:26.5729\n",
      "#71, train loss:24.9664 val loss:26.5764 no improve...\n",
      "#72, train loss:24.9475 val loss:26.5563\n",
      "#73, train loss:24.9269 val loss:26.5145\n",
      "#74, train loss:24.9006 val loss:26.4781\n",
      "#75, train loss:24.8788 val loss:26.4660\n",
      "#76, train loss:24.8674 val loss:26.4366\n",
      "#77, train loss:24.8500 val loss:26.4103\n",
      "#78, train loss:24.8308 val loss:26.4051\n",
      "#79, train loss:24.8153 val loss:26.4039\n",
      "#80, train loss:24.8059 val loss:26.3931\n",
      "#81, train loss:24.7960 val loss:26.3638\n",
      "#82, train loss:24.7807 val loss:26.3382\n",
      "#83, train loss:24.7701 val loss:26.3244\n",
      "#84, train loss:24.7577 val loss:26.3347 no improve...\n",
      "#85, train loss:24.7472 val loss:26.3292 no improve...\n",
      "#86, train loss:24.7379 val loss:26.2980\n",
      "#87, train loss:24.7255 val loss:26.2868\n",
      "#88, train loss:24.7195 val loss:26.2879 no improve...\n",
      "#89, train loss:24.7097 val loss:26.2939 no improve...\n",
      "#90, train loss:24.7032 val loss:26.2841\n",
      "#91, train loss:24.6953 val loss:26.2671\n",
      "#92, train loss:24.6892 val loss:26.2613\n",
      "#93, train loss:24.6833 val loss:26.2683 no improve...\n",
      "#94, train loss:24.6767 val loss:26.2719 no improve...\n",
      "#95, train loss:24.6716 val loss:26.2570\n",
      "#96, train loss:24.6645 val loss:26.2485\n",
      "#97, train loss:24.6588 val loss:26.2538 no improve...\n",
      "#98, train loss:24.6517 val loss:26.2611 no improve...\n",
      "#99, train loss:24.6473 val loss:26.2416\n",
      "#100, train loss:24.6411 val loss:26.2343\n",
      "#101, train loss:24.6356 val loss:26.2441 no improve...\n",
      "#102, train loss:24.6296 val loss:26.2285\n",
      "#103, train loss:24.6231 val loss:26.2141\n",
      "#104, train loss:24.6190 val loss:26.1992\n",
      "#105, train loss:24.6126 val loss:26.3275 no improve...\n",
      "#106, train loss:24.6567 val loss:26.2056 no improve...\n",
      "#107, train loss:24.6833 val loss:26.2163 no improve...\n",
      "#108, train loss:24.6014 val loss:26.2889 no improve...\n",
      "#109, train loss:24.6355 val loss:26.1983\n",
      "#110, train loss:24.6007 val loss:26.1882\n",
      "#111, train loss:24.6265 val loss:26.2032 no improve...\n",
      "#112, train loss:24.5980 val loss:26.2616 no improve...\n",
      "#113, train loss:24.6191 val loss:26.2035 no improve...\n",
      "#114, train loss:24.5942 val loss:26.1854\n",
      "#115, train loss:24.6093 val loss:26.1953 no improve...\n",
      "#116, train loss:24.5889 val loss:26.2449 no improve...\n",
      "#117, train loss:24.5995 val loss:26.2094 no improve...\n",
      "#118, train loss:24.5832 val loss:26.1820\n",
      "#119, train loss:24.5885 val loss:26.1885 no improve...\n",
      "#120, train loss:24.5757 val loss:26.2295 no improve...\n",
      "#121, train loss:24.5796 val loss:26.2070 no improve...\n",
      "#122, train loss:24.5692 val loss:26.1798\n",
      "#123, train loss:24.5717 val loss:26.1858 no improve...\n",
      "#124, train loss:24.5628 val loss:26.2193 no improve...\n",
      "#125, train loss:24.5656 val loss:26.1820 no improve...\n",
      "#126, train loss:24.5544 val loss:26.1630\n",
      "#127, train loss:24.5559 val loss:26.1733 no improve...\n",
      "#128, train loss:24.5482 val loss:26.1852 no improve...\n",
      "#129, train loss:24.5481 val loss:26.1577\n",
      "#130, train loss:24.5415 val loss:26.1486\n",
      "#131, train loss:24.5410 val loss:26.1654 no improve...\n",
      "#132, train loss:24.5362 val loss:26.1628 no improve...\n",
      "#133, train loss:24.5328 val loss:26.1385\n",
      "#134, train loss:24.5300 val loss:26.1394 no improve...\n",
      "#135, train loss:24.5259 val loss:26.1547 no improve...\n",
      "#136, train loss:24.5243 val loss:26.1367\n",
      "#137, train loss:24.5192 val loss:26.1224\n",
      "#138, train loss:24.5174 val loss:26.1303 no improve...\n",
      "#139, train loss:24.5112 val loss:26.1070\n",
      "#140, train loss:24.5073 val loss:26.1112 no improve...\n",
      "#141, train loss:24.5044 val loss:26.0931\n",
      "#142, train loss:24.5017 val loss:26.1053 no improve...\n",
      "#143, train loss:24.4982 val loss:26.0808\n",
      "#144, train loss:24.4963 val loss:26.1625 no improve...\n",
      "#145, train loss:24.5102 val loss:26.1135 no improve...\n",
      "#146, train loss:24.6244 val loss:26.5109 no improve...\n",
      "#147, train loss:24.7530 val loss:26.0772\n",
      "#148, train loss:24.5583 val loss:26.0757\n",
      "#149, train loss:24.5621 val loss:26.2517 no improve...\n",
      "#150, train loss:24.5844 val loss:26.1748 no improve...\n",
      "#151, train loss:24.5313 val loss:26.0853 no improve...\n",
      "#152, train loss:24.5647 val loss:26.0690\n",
      "#153, train loss:24.5096 val loss:26.2172 no improve...\n",
      "#154, train loss:24.5439 val loss:26.1680 no improve...\n",
      "#155, train loss:24.5120 val loss:26.0915 no improve...\n",
      "#156, train loss:24.5209 val loss:26.0875 no improve...\n",
      "#157, train loss:24.5150 val loss:26.1452 no improve...\n",
      "#158, train loss:24.4967 val loss:26.1798 no improve...\n",
      "#159, train loss:24.5161 val loss:26.0793 no improve...\n",
      "#160, train loss:24.4817 val loss:26.0766 no improve...\n",
      "#161, train loss:24.5116 val loss:26.0901 no improve...\n",
      "#162, train loss:24.4733 val loss:26.1593 no improve...\n",
      "#163, train loss:24.5015 val loss:26.0908 no improve...\n",
      "#164, train loss:24.4693 val loss:26.0652\n",
      "#165, train loss:24.4890 val loss:26.0672 no improve...\n",
      "#166, train loss:24.4675 val loss:26.1253 no improve...\n",
      "#167, train loss:24.4778 val loss:26.0941 no improve...\n",
      "#168, train loss:24.4626 val loss:26.0453\n",
      "#169, train loss:24.4694 val loss:26.0511 no improve...\n",
      "#170, train loss:24.4501 val loss:26.1107 no improve...\n",
      "#171, train loss:24.4613 val loss:26.0614 no improve...\n",
      "#172, train loss:24.4427 val loss:26.0354\n",
      "#173, train loss:24.4539 val loss:26.0605 no improve...\n",
      "#174, train loss:24.4364 val loss:26.0887 no improve...\n",
      "#175, train loss:24.4432 val loss:26.0269\n",
      "#176, train loss:24.4391 val loss:26.0302 no improve...\n",
      "#177, train loss:24.4307 val loss:26.0759 no improve...\n",
      "#178, train loss:24.4365 val loss:26.0232\n",
      "#179, train loss:24.4257 val loss:26.0106\n",
      "#180, train loss:24.4285 val loss:26.0486 no improve...\n",
      "#181, train loss:24.4242 val loss:26.0268 no improve...\n",
      "#182, train loss:24.4169 val loss:26.0023\n",
      "#183, train loss:24.4201 val loss:26.0405 no improve...\n",
      "#184, train loss:24.4145 val loss:26.0159 no improve...\n",
      "#185, train loss:24.4066 val loss:25.9982\n",
      "#186, train loss:24.4074 val loss:26.0374 no improve...\n",
      "#187, train loss:24.4051 val loss:25.9840\n",
      "#188, train loss:24.4043 val loss:26.0321 no improve...\n",
      "#189, train loss:24.3992 val loss:25.9757\n",
      "#190, train loss:24.3977 val loss:26.0275 no improve...\n",
      "#191, train loss:24.3948 val loss:25.9693\n",
      "#192, train loss:24.3960 val loss:26.0338 no improve...\n",
      "#193, train loss:24.3950 val loss:25.9657\n",
      "#194, train loss:24.3967 val loss:26.0202 no improve...\n",
      "#195, train loss:24.3871 val loss:25.9664 no improve...\n",
      "#196, train loss:24.3781 val loss:25.9690 no improve...\n",
      "#197, train loss:24.3742 val loss:25.9947 no improve...\n",
      "#198, train loss:24.3767 val loss:25.9464\n",
      "#199, train loss:24.3788 val loss:25.9896 no improve...\n",
      "#200, train loss:24.3735 val loss:25.9498 no improve...\n",
      "#201, train loss:24.3658 val loss:25.9465 no improve...\n",
      "#202, train loss:24.3638 val loss:25.9765 no improve...\n",
      "#203, train loss:24.3656 val loss:25.9389\n",
      "#204, train loss:24.3626 val loss:25.9538 no improve...\n",
      "#205, train loss:24.3569 val loss:25.9612 no improve...\n",
      "#206, train loss:24.3563 val loss:25.9373\n",
      "#207, train loss:24.3566 val loss:25.9588 no improve...\n",
      "#208, train loss:24.3532 val loss:25.9501 no improve...\n",
      "#209, train loss:24.3504 val loss:25.9380 no improve...\n",
      "#210, train loss:24.3508 val loss:25.9574 no improve...\n",
      "#211, train loss:24.3493 val loss:25.9420 no improve...\n",
      "#212, train loss:24.3464 val loss:25.9342\n",
      "#213, train loss:24.3458 val loss:25.9491 no improve...\n",
      "#214, train loss:24.3449 val loss:25.9311\n",
      "#215, train loss:24.3425 val loss:25.9307\n",
      "#216, train loss:24.3411 val loss:25.9403 no improve...\n",
      "#217, train loss:24.3406 val loss:25.9225\n",
      "#218, train loss:24.3394 val loss:25.9324 no improve...\n",
      "#219, train loss:24.3375 val loss:25.9275 no improve...\n",
      "#220, train loss:24.3360 val loss:25.9193\n",
      "#221, train loss:24.3352 val loss:25.9312 no improve...\n",
      "#222, train loss:24.3343 val loss:25.9123\n",
      "#223, train loss:24.3331 val loss:25.9243 no improve...\n",
      "#224, train loss:24.3316 val loss:25.9119\n",
      "#225, train loss:24.3299 val loss:25.9139 no improve...\n",
      "#226, train loss:24.3286 val loss:25.9151 no improve...\n",
      "#227, train loss:24.3274 val loss:25.9028\n",
      "#228, train loss:24.3266 val loss:25.9174 no improve...\n",
      "#229, train loss:24.3258 val loss:25.8920\n",
      "#230, train loss:24.3264 val loss:25.9324 no improve...\n",
      "#231, train loss:24.3297 val loss:25.8830\n",
      "#232, train loss:24.3376 val loss:25.9846 no improve...\n",
      "#233, train loss:24.3562 val loss:25.9002 no improve...\n",
      "#234, train loss:24.3927 val loss:26.0979 no improve...\n",
      "#235, train loss:24.4311 val loss:25.9101 no improve...\n",
      "#236, train loss:24.4123 val loss:25.9739 no improve...\n",
      "#237, train loss:24.3483 val loss:25.9165 no improve...\n",
      "#238, train loss:24.3215 val loss:25.8918 no improve...\n",
      "#239, train loss:24.3625 val loss:25.9968 no improve...\n",
      "#240, train loss:24.3602 val loss:25.8988 no improve...\n",
      "#241, train loss:24.3179 val loss:25.8898 no improve...\n",
      "#242, train loss:24.3433 val loss:25.9925 no improve...\n",
      "#243, train loss:24.3519 val loss:25.8968 no improve...\n",
      "#244, train loss:24.3174 val loss:25.8895 no improve...\n",
      "#245, train loss:24.3279 val loss:25.9793 no improve...\n",
      "#246, train loss:24.3435 val loss:25.8890 no improve... early stop...\n",
      "test loss : {} 25.1235408782959\n"
     ]
    }
   ],
   "source": [
    "early_stop = 15\n",
    "min_val_loss = 999999\n",
    "min_val_epoch = 0\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    logits = model(graph, node_features)\n",
    "    \n",
    "    train_loss = F.mse_loss(logits[train_mask], node_labels[train_mask])\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    train_loss.backward()\n",
    "    opt.step()\n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    val_loss = evaluate(model, graph, node_features, node_labels, valid_mask)\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    print('#{}, train loss:{:.4f} val loss:{:.4f}'.format(epoch,train_loss.item(),val_loss.item()),end='')\n",
    "    \n",
    "    if val_loss.item() < min_val_loss:\n",
    "        min_val_loss = val_loss.item()\n",
    "        min_val_epoch = epoch\n",
    "    else:\n",
    "        print(' no improve...',end='')\n",
    "        \n",
    "    if epoch >= min_val_epoch + early_stop :\n",
    "        print(' early stop...')\n",
    "        break\n",
    "        \n",
    "\n",
    "    print()\n",
    "    # Save model if necessary.  Omitted in this example.\n",
    "    \n",
    "test_loss = evaluate(model, graph, node_features, node_labels, test_mask)\n",
    "print('test loss : {}',test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d55e87fc-9bd5-46cb-b8a4-09e17b34426f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16b1e999548>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses,label='train')\n",
    "plt.plot(val_losses,label='validation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4617c-467c-4fd0-8462-937499be20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "25.1235408782959"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
